{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from datetime import datetime\n",
    "from env import DiscreteEnv\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/20241121-20251121 CAISO Real-time Price.csv\") # for training\n",
    "df = df[df.hub == \"TH_SP15\"].reset_index(drop=True)\n",
    "df.Date = [datetime.strptime(date.strip(\"'\"), \"%m/%d/%Y %I:%M:%S %p\") for date in df.Date]\n",
    "df_quarterhr = df.iloc[[i for i in range(len(df.Date)) if df.Date.iloc[i].minute in [0, 15, 30, 45]]]\n",
    "\n",
    "df2 = pd.read_csv(\"../data/20231121-20241121 CAISO Real-time Price.csv\") # for testing\n",
    "df2 = df2[df2.hub == \"TH_SP15\"].reset_index(drop=True)\n",
    "df2.Date = [datetime.strptime(date.strip(\"'\"), \"%m/%d/%Y %I:%M:%S %p\") for date in df2.Date]\n",
    "df2_quarterhr = df2.iloc[[i for i in range(len(df2.Date)) if df2.Date.iloc[i].minute in [0, 15, 30, 45]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_quarterhr.price)\n",
    "plt.plot(df2_quarterhr.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteEnv(4*24*50, df_quarterhr)\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "cum_rew = [0]\n",
    "soc = [0]\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample() # random baseline\n",
    "    state, reward, done, _, info = env.step(action)\n",
    "    cum_rew.append(cum_rew[-1]+reward)\n",
    "    soc.append(state[0])\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(range(len(cum_rew)), cum_rew)\n",
    "#plt.plot(range(len(soc)), soc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN PARAMS\n",
    "RL_PARAMS = {\n",
    "    'policy': \"MlpPolicy\",\n",
    "    # 'learning_rate': 0.00176746728919149,  # Default: 1e-4\n",
    "    'learning_rate': 1e-4,\n",
    "    'buffer_size': 100_000,  # Default: 1e6\n",
    "    'learning_starts': 255,  # Default: 50_000\n",
    "    'batch_size': 256,  # Default: 32\n",
    "    'tau': 0.5016120493544259,  # Default: 1.0\n",
    "    'gamma': 0.9999812912592504,  # Default: 0.99\n",
    "    'train_freq': 84,  # Default: 4\n",
    "    'gradient_steps': -1,  # Default: 1\n",
    "    'target_update_interval': 10_000,  # Default: 1e4\n",
    "    'exploration_fraction': 0.5,  # Default: 0.1\n",
    "    'exploration_initial_eps': 1.0,  # Default: 1.0\n",
    "    'exploration_final_eps': 0.005,  # Default: 0.05\n",
    "    'max_grad_norm': 3.266151433390378,  # Default: 10\n",
    "\n",
    "    'policy_kwargs': {\n",
    "        # Defaults reported for MultiInputPolicy\n",
    "        'net_arch': 'extra_large',  # Default: None\n",
    "        'activation_fn': 'leaky_relu',  # Default: tanh\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "mean_reward: 290242.92 +/- 1223.03\n",
      "\n",
      "Execution time = 171.53819513320923s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "from stable_baselines3 import PPO, SAC, DDPG, DQN, A2C #type: ignore\n",
    "from stable_baselines3.common.callbacks import EvalCallback #type: ignore\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #type: ignore\n",
    "from stable_baselines3.common.logger import configure #type: ignore\n",
    "\n",
    "\"\"\"\n",
    "    Trains a reinforcement learning agent.\n",
    "\n",
    "    :param agent: A string that represents the name of the reinforcement learning agent.\n",
    "    :param run: An integer that represents the run number.\n",
    "    :param path: A string that represents the path to the directory where the data will be saved.\n",
    "    :param exp_params: A dictionary that contains the experiment parameters.\n",
    "    :param env_id: A string that represents the ID of the environment.\n",
    "    :param env_kwargs: A dictionary that contains the keyword arguments for the environment.\n",
    "    :param rl_params: A dictionary that contains the reinforcement learning parameters.\n",
    "    :param verbose: An integer that represents the verbosity level.\n",
    "    :param discrete_actions: A list of discrete actions.\n",
    "    :param logger_type: A list that represents the logger type.\n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# CREATE ENVIRONMENT\n",
    "env = DiscreteEnv(8760, df_quarterhr)\n",
    "    \n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "\n",
    "eval_env = DiscreteEnv(8760*4, df2_quarterhr) # corresponds to 8760 hours from the og paper\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print()\n",
    "print(f'Execution time = {time.time() - start}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
